# automated-preferential-looking
This project aims to develop a ready-to-deploy application suite that will address these limitations by integrating hardware devices or deep learning-based infant eye trackers, and visual stimuli analysis into a user-friendly graphical user interface (GUI).

## Steps to test the program:
### To test the gui:
* Navigate to the cloned directory:
```sh
cd automated-preferential-looking
```

* Get the latest changes in the local repo:
```sh
git pull
```

* Also clone all the newly added branches:
```sh
git fetch
```
* Change to the final directory to test the gui:
```sh
git checkout final
```
* Now run the main.py file:
```sh
python main.py
```
### To test the icatcher integration with the dummy experiment:
* Navigate to the cloned directory:
```sh
cd automated-preferential-looking
```

* Get the latest changes in the local repo:
```sh
git pull
```

* Also clone all the newly added branches:
```sh
git fetch
```
* Change to the final directory to test the gui:
```sh
git checkout icatcher-integration
```
* Install the required dependencies:
```sh
pip install icatcher
```
```sh
pip install ffmpeg-python   
```
* Now run the main.py file:
```sh
python main.py
```
